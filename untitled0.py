# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TJ3WDImysP-ECrn6sl8Y3u8eMZh5frND
"""

!pip install catboost

# ================ IMPORTS ================
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import VotingClassifier, RandomForestClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
# Initialize LabelEncoder here to make it globally accessible
label_encoder = LabelEncoder()

# ================ DATA LOADING ================
train_df = pd.read_csv("UNSW_NB15_training-set.csv")
test_df = pd.read_csv("UNSW_NB15_testing-set.csv")

# ================ PREPROCESSING ================

def preprocess(df):
    df = df.copy()

    # Drop ID column if exists
    df.drop(columns=['id'], errors='ignore', inplace=True)

    # Label encode categorical columns
    cat_cols = df.select_dtypes(include='object').columns
    for col in cat_cols:
        df[col] = label_encoder.fit_transform(df[col].astype(str))

    return df

train_df = preprocess(train_df)
test_df = preprocess(test_df)

# Split features/labels
X_train = train_df.drop(columns=['label'])
y_train = train_df['label']
X_test = test_df.drop(columns=['label'])
y_test = test_df['label']

# Standardize
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# ================ MODEL TRAINING ================
def train_models_gpu(X_train, y_train):
    print(" Training Random Forest (CPU fallback)...")
    rf = RandomForestClassifier(
        n_estimators=500,
        max_depth=30,
        class_weight='balanced',
        n_jobs=-1,
        random_state=42,
        verbose=1
    )
    rf.fit(X_train, y_train)

    print("\n Training XGBoost (GPU)...")
    xgb_params = {
        'n_estimators': [500, 600],
        'learning_rate': [0.05, 0.1],
        'max_depth': [8, 10],
        'subsample': [0.8, 1.0],
        'colsample_bytree': [0.8, 1.0],
        'tree_method': ['gpu_hist'],
        'predictor': ['gpu_predictor'],
        'enable_categorical': [False]
    }

    xgb = XGBClassifier(
        n_jobs=-1,
        eval_metric='mlogloss',
        random_state=42,
        use_label_encoder=False
    )

    xgb_search = RandomizedSearchCV(
        xgb,
        xgb_params,
        n_iter=10,
        cv=3,
        scoring='f1_weighted',
        n_jobs=-1,
        verbose=1
    )
    xgb_search.fit(X_train, y_train)
    best_xgb = xgb_search.best_estimator_
    print(f" Best XGB params: {xgb_search.best_params_}")

    print("\n Training CatBoost (GPU)...")
    cat = CatBoostClassifier(
        iterations=1000,
        learning_rate=0.05,
        depth=10,
        l2_leaf_reg=3,
        random_seed=42,
        verbose=100,
        early_stopping_rounds=50,
        task_type='GPU'
    )
    cat.fit(X_train, y_train)

    return rf, best_xgb, cat

rf_model, xgb_model, cat_model = train_models_gpu(X_train, y_train)

# ================ ENSEMBLE ================
def create_and_evaluate_ensemble(rf, xgb, cat, X_test, y_test):
    ensemble = VotingClassifier(
        estimators=[('rf', rf), ('xgb', xgb), ('cat', cat)],
        voting='soft',
        weights=[1, 2, 2]
    )
    ensemble.fit(X_train, y_train)

    y_pred = ensemble.predict(X_test)
    print("\n Ensemble Performance:")
    print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))

    plt.figure(figsize=(10, 8))
    sns.heatmap(
        confusion_matrix(y_test, y_pred),
        annot=True,
        fmt='d',
        cmap='Blues'
    )
    plt.title('Ensemble Confusion Matrix')
    plt.show()

    return ensemble

final_model = create_and_evaluate_ensemble(rf_model, xgb_model, cat_model, X_test, y_test)

# ================ SAVE ARTIFACTS ================
def save_artifacts(model, scaler):
    joblib.dump(model, "intrusion_detection_ensemble.pkl")
    joblib.dump(scaler, "scaler.pkl")
    joblib.dump(label_encoder, "label_encoder.pkl")
    print(" All artifacts saved successfully!")

save_artifacts(final_model, scaler)